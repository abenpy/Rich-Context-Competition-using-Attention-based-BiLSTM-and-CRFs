{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do:\n",
    "## for every paper:\n",
    "## 1. take only from abstract -- can't apply: papers with no abstract, e.g. 176\n",
    "## 2. break up into sections: abstract, introduction, methods, results, discussion, conclussion, references\n",
    "        ## lines with less than 5 char\n",
    "        ## new paragraph symbol: \\x0c\n",
    "## 3. remove new line symbols;\n",
    "## 4. break up into sentences in each sentence\n",
    "## 5. match mention lists\n",
    "## 6. tokenize\n",
    "## 6. tag other words\n",
    "\n",
    "## output - columns: sentence number, words, POS, tag, case sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import json\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = sys.argv[0]\n",
    "# data_path = \"../data/input/\"\n",
    "data_path = \"../../train_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files = glob.glob('files/text/*.txt')\n",
    "# all_files.sort()\n",
    "# # all_files\n",
    "with open(data_path+'/publications.json') as f:\n",
    "    sample_dict = json.load(f)\n",
    "filehandles = [str(i['publication_id']) for i in sample_dict]\n",
    "# filehandles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df.loc[labels_df['publication_id'] == int(handle)]['mention_list'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle = '105'\n",
    "# with open(data_path+'/files/text/'+handle+'.txt', 'r') as f:\n",
    "#     txt_test_raw = f.read()\n",
    "# print(\"Processing:\", handle)\n",
    "# txt_test_parsed = [i for i in txt_test_raw.split('\\x0c')]\n",
    "# txt_test_rm_paragraph = ''.join(txt_test_parsed)\n",
    "# txt_test_connect_word = txt_test_rm_paragraph.replace('-\\n', '')\n",
    "# txt_test_connect_word\n",
    "# # txt_test_parsed_filter = [i for i in txt_test_rm_paragraph.split('\\n') if len(i)>3]\n",
    "# # txt_test_parsed_concat = ' '.join(txt_test_parsed_filter)\n",
    "# # txt_test_sentences = tokenize.sent_tokenize(txt_test_parsed_concat)\n",
    "# # tokenizer = tokenize.RegexpTokenizer(r'\\w+')\n",
    "# # txt_tokenize = [tokenizer.tokenize(i) for i in txt_test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_len = [len(i) for i in txt_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(sent_len), np.std(sent_len), np.median(sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38671990"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat = pd.read_csv('../../train_test/df_concat.csv')\n",
    "len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_len = df_concat.groupby(['Sentence_ID']).agg(['count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1913592"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len = np.array(df_sentence_len.iloc[:,0])\n",
    "len(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len_filter = sent_len[sent_len<200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len_clip = [i if i <= 200 else 200 for i in sent_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sent_len_clip, bins=100)\n",
    "plt.axvline(x=30, color = 'black', linestyle = 'dashed', linewidth = 1)\n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle = '170'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path+'/files/text/'+handle+'.txt', 'r') as f:\n",
    "#     txt_test_raw = f.read()\n",
    "# print(\"Processing:\", handle)\n",
    "# txt_test_parsed = [i for i in txt_test_raw.split('\\x0c')]\n",
    "# txt_test_rm_paragraph = ''.join(txt_test_parsed)\n",
    "# txt_test_connect_word = txt_test_rm_paragraph.replace('-\\n', '- ')\n",
    "# txt_test_parsed_filter = [i for i in txt_test_connect_word.split('\\n') if len(i)>3]\n",
    "# txt_test_parsed_concat = ' '.join(txt_test_parsed_filter)\n",
    "# txt_test_sentences = tokenize.sent_tokenize(txt_test_parsed_concat)\n",
    "# mention_lists = labels_df.loc[labels_df['publication_id'] == int(handle)]['mention_list'] \n",
    "# sentence_idx = {}\n",
    "# for mention_list in mention_lists:\n",
    "#     if mention_list == []:\n",
    "#         continue\n",
    "#     else:\n",
    "#         for phrase in mention_list:\n",
    "#             sentence_idx[phrase] = [txt_test_sentences.index(s) for s in txt_test_sentences if phrase in s]\n",
    "# mention_sentences_id = list(set([x for y in sentence_idx.values() for x in y]))\n",
    "# mention_sentences_id.sort()\n",
    "# tokenizer = tokenize.RegexpTokenizer(r'\\w+')\n",
    "# txt_tokenize = [tokenizer.tokenize(i) for i in txt_test_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_sentence = 0\n",
    "# last_sentence_norm = 0\n",
    "# for i in range(len(txt_tokenize)):\n",
    "#     pos_raw = nltk.pos_tag(txt_tokenize[i])\n",
    "#     pos = [i[1] for i in pos_raw]\n",
    "#     words = txt_tokenize[i]\n",
    "#     sent_count = int(np.ceil(len(words)/max_sent_len))\n",
    "#     word_ul = []\n",
    "#     for j in range(len(words)):\n",
    "#         word_ul.append(''.join(['U' if x.isupper() else 'L' for x in words[j]]))\n",
    "#     if i == 0:\n",
    "#         text_df = pd.DataFrame({\"Word_ID\": range(len(txt_tokenize[i])), \"Sentence_ID\": i+last_sentence, \n",
    "#                                 \"Word_ID_Norm\": range(len(txt_tokenize[i])), \"Sentence_ID_Norm\": i+last_sentence_norm,\n",
    "#                                 \"Pub_id\": handle, \n",
    "#                                 \"Word\": txt_tokenize[i], \"POS\": pos, \"UL\": word_ul})\n",
    "#         if sent_count > 1:\n",
    "#             for j in range(sent_count)[1:]:\n",
    "#                 sent_len = np.min([max_sent_len, text_df.shape[0]-max_sent_len*j])\n",
    "#                 text_df.iloc[j*max_sent_len+np.array(range(sent_len)), 2] = np.array(range(sent_len))\n",
    "#                 text_df.iloc[j*max_sent_len+np.array(range(sent_len)), 3] += j\n",
    "#             last_sentence_norm += j\n",
    "# #                 print(text_df)\n",
    "#         if i in mention_sentences_id:\n",
    "#             text_df['Tag'] = 'M'\n",
    "#         else:\n",
    "#             text_df['Tag'] = 'O'\n",
    "\n",
    "#     else:\n",
    "#         text_df_add = pd.DataFrame({\"Word_ID\": range(len(txt_tokenize[i])), \"Sentence_ID\": i+last_sentence, \n",
    "#                                 \"Word_ID_Norm\": range(len(txt_tokenize[i])), \"Sentence_ID_Norm\": i+last_sentence_norm,\n",
    "#                                 \"Pub_id\": handle, \n",
    "#                                 \"Word\": txt_tokenize[i], \"POS\": pos, \"UL\": word_ul})\n",
    "#         if sent_count > 1:\n",
    "#             for j in range(sent_count)[1:]:\n",
    "#                 sent_len = np.min([max_sent_len, text_df_add.shape[0]-max_sent_len*j])\n",
    "#                 text_df_add.iloc[j*max_sent_len+np.array(range(sent_len)), 2] = np.array(range(sent_len))\n",
    "#                 text_df_add.iloc[j*max_sent_len+np.array(range(sent_len)), 3] += j\n",
    "#             last_sentence_norm += j\n",
    "# #                 print(text_df_add)\n",
    "#         if i in mention_sentences_id:\n",
    "#             text_df_add['Tag'] = 'M'\n",
    "#         else:\n",
    "#             text_df_add['Tag'] = 'O'\n",
    "#         text_df = pd.concat([text_df,text_df_add], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mention_phrases = list(sentence_idx.keys())\n",
    "# for mention_phrase in mention_phrases:\n",
    "#     print(mention_phrase)\n",
    "#     mention_token = tokenizer.tokenize(mention_phrase)\n",
    "#     print(sentence_idx[mention_phrase])\n",
    "#     for sentence_id in sentence_idx[mention_phrase]:\n",
    "#         print(sentence_id)\n",
    "#         mentioned_sentence = list(text_df['Word'][text_df['Sentence_ID'] == sentence_id+last_sentence])\n",
    "#         print(mentioned_sentence)\n",
    "#         match_idx = []\n",
    "#         for ref_token_idx in range(len(mention_token)):\n",
    "#             for sentence_token_idx in range(len(mentioned_sentence)):\n",
    "#                 if mentioned_sentence[sentence_token_idx] == mention_token[ref_token_idx]:\n",
    "#                     match_idx.append(sentence_token_idx)\n",
    "#         match_idx = list(set(match_idx))\n",
    "#         match_idx.sort()\n",
    "#         print(match_idx)\n",
    "#         match_idx_final = []\n",
    "#         for word_idx in match_idx:\n",
    "#             if match_idx_final == []:\n",
    "#                 match_idx_final.append(word_idx)\n",
    "#             else:\n",
    "#                 if word_idx == match_idx_final[-1]+1:\n",
    "#                     match_idx_final.append(word_idx)\n",
    "#                 elif len(match_idx_final) == len(mention_token):\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     match_idx_final = [word_idx]\n",
    "#         print(match_idx_final)\n",
    "#         tag = [\"B\"] + [\"I\"] * (len(match_idx_final)-1)\n",
    "#         print(tag)\n",
    "#         if len(match_idx_final)>1:\n",
    "#             text_df.loc[(text_df['Sentence_ID'] == sentence_id+last_sentence) & \n",
    "#                         (text_df['Word_ID'] >= match_idx_final[0]) & \n",
    "#                         (text_df['Word_ID'] <= match_idx_final[-1]), 'Tag'] = tag\n",
    "#             print(text_df.loc[text_df['Sentence_ID'] == sentence_id])\n",
    "\n",
    "#         elif len(match_idx_final) == 1:\n",
    "#             text_df.loc[(text_df['Sentence_ID'] == sentence_id+last_sentence) & \n",
    "#                         (text_df['Word_ID'] == match_idx_final[0]), 'Tag'] = tag   \n",
    "#             print(text_df.loc[(text_df['Sentence_ID'] == sentence_id) & \n",
    "#                         (text_df['Word_ID'] == match_idx_final[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_df[text_df['Sentence_ID'] == 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path+'/data_set_citations.json') as f:\n",
    "with open('../rich-context-competition/evaluate/data_set_citations.json') as f:  \n",
    "    labels_raw = json.load(f)\n",
    "labels_df = pd.DataFrame(labels_raw)\n",
    "\n",
    "data_tidy_dict = {}\n",
    "last_sentence = 0\n",
    "last_sentence_norm = 0\n",
    "max_sent_len = 30\n",
    "for handle in filehandles:\n",
    "    with open(data_path+'/files/text/'+handle+'.txt', 'r') as f:\n",
    "        txt_test_raw = f.read()\n",
    "    print(\"Processing:\", handle)\n",
    "    txt_test_parsed = [i for i in txt_test_raw.split('\\x0c')]\n",
    "    txt_test_rm_paragraph = ''.join(txt_test_parsed)\n",
    "    txt_test_connect_word = txt_test_rm_paragraph.replace('-\\n', '- ')\n",
    "    txt_test_parsed_filter = [i for i in txt_test_connect_word.split('\\n') if len(i)>3]\n",
    "    txt_test_parsed_concat = ' '.join(txt_test_parsed_filter)\n",
    "    txt_test_sentences = tokenize.sent_tokenize(txt_test_parsed_concat)\n",
    "    mention_lists = labels_df.loc[labels_df['publication_id'] == int(handle)]['mention_list'] \n",
    "    sentence_idx = {}\n",
    "    for mention_list in mention_lists:\n",
    "        if mention_list == []:\n",
    "            continue\n",
    "        else:\n",
    "            for phrase in mention_list:\n",
    "                sentence_idx[phrase] = [txt_test_sentences.index(s) for s in txt_test_sentences if phrase in s]\n",
    "    mention_sentences_id = list(set([x for y in sentence_idx.values() for x in y]))\n",
    "    mention_sentences_id.sort()\n",
    "    tokenizer = tokenize.RegexpTokenizer(r'\\w+')\n",
    "    txt_tokenize = [tokenizer.tokenize(i) for i in txt_test_sentences]\n",
    "    for i in range(len(txt_tokenize)):\n",
    "        pos_raw = nltk.pos_tag(txt_tokenize[i])\n",
    "        pos = [i[1] for i in pos_raw]\n",
    "        words = txt_tokenize[i]\n",
    "        sent_count = int(np.ceil(len(words)/max_sent_len))\n",
    "        word_ul = []\n",
    "        for j in range(len(words)):\n",
    "            word_ul.append(''.join(['U' if x.isupper() else 'L' for x in words[j]]))\n",
    "        if i == 0:\n",
    "            text_df = pd.DataFrame({\"Word_ID\": range(len(txt_tokenize[i])), \"Sentence_ID\": i+last_sentence, \n",
    "                                    \"Word_ID_Norm\": range(len(txt_tokenize[i])), \"Sentence_ID_Norm\": i+last_sentence_norm,\n",
    "                                    \"Pub_id\": handle, \n",
    "                                    \"Word\": txt_tokenize[i], \"POS\": pos, \"UL\": word_ul})\n",
    "            if sent_count > 1:\n",
    "                for j in range(sent_count)[1:]:\n",
    "                    sent_len = np.min([max_sent_len, text_df.shape[0]-max_sent_len*j])\n",
    "                    text_df.iloc[j*max_sent_len+np.array(range(sent_len)), 2] = np.array(range(sent_len))\n",
    "                    text_df.iloc[j*max_sent_len+np.array(range(sent_len)), 3] += j\n",
    "                last_sentence_norm += j\n",
    "#                 print(text_df)\n",
    "            if i in mention_sentences_id:\n",
    "                text_df['Tag'] = 'M'\n",
    "            else:\n",
    "                text_df['Tag'] = 'O'\n",
    "        \n",
    "        else:\n",
    "            text_df_add = pd.DataFrame({\"Word_ID\": range(len(txt_tokenize[i])), \"Sentence_ID\": i+last_sentence, \n",
    "                                    \"Word_ID_Norm\": range(len(txt_tokenize[i])), \"Sentence_ID_Norm\": i+last_sentence_norm,\n",
    "                                    \"Pub_id\": handle, \n",
    "                                    \"Word\": txt_tokenize[i], \"POS\": pos, \"UL\": word_ul})\n",
    "            if sent_count > 1:\n",
    "                for j in range(sent_count)[1:]:\n",
    "                    sent_len = np.min([max_sent_len, text_df_add.shape[0]-max_sent_len*j])\n",
    "                    text_df_add.iloc[j*max_sent_len+np.array(range(sent_len)), 2] = np.array(range(sent_len))\n",
    "                    text_df_add.iloc[j*max_sent_len+np.array(range(sent_len)), 3] += j\n",
    "                last_sentence_norm += j\n",
    "#                 print(text_df_add)\n",
    "            if i in mention_sentences_id:\n",
    "                text_df_add['Tag'] = 'M'\n",
    "            else:\n",
    "                text_df_add['Tag'] = 'O'\n",
    "            text_df = pd.concat([text_df,text_df_add], axis=0)\n",
    "    mention_phrases = list(sentence_idx.keys())\n",
    "    for mention_phrase in mention_phrases:\n",
    "        mention_token = tokenizer.tokenize(mention_phrase)\n",
    "        for sentence_id in sentence_idx[mention_phrase]:\n",
    "#             print(sentence_id)\n",
    "            mentioned_sentence = list(text_df['Word'][text_df['Sentence_ID'] == sentence_id+last_sentence])\n",
    "#             print(mentioned_sentence)\n",
    "            match_idx = []\n",
    "            for ref_token_idx in range(len(mention_token)):\n",
    "                for sentence_token_idx in range(len(mentioned_sentence)):\n",
    "                    if mentioned_sentence[sentence_token_idx] == mention_token[ref_token_idx]:\n",
    "                        match_idx.append(sentence_token_idx)\n",
    "            match_idx = list(set(match_idx))\n",
    "            match_idx.sort()\n",
    "#             print(match_idx)\n",
    "            match_idx_final = []\n",
    "            for word_idx in match_idx:\n",
    "                if match_idx_final == []:\n",
    "                    match_idx_final.append(word_idx)\n",
    "                else:\n",
    "                    if word_idx == match_idx_final[-1]+1:\n",
    "                        match_idx_final.append(word_idx)\n",
    "                    elif len(match_idx_final) == len(mention_token):\n",
    "                        break\n",
    "                    else:\n",
    "                        match_idx_final = [word_idx]\n",
    "#             print(match_idx_final)\n",
    "            tag = [\"B\"] + [\"I\"] * (len(match_idx_final)-1)\n",
    "#             print(tag)\n",
    "            if len(match_idx_final)>1:\n",
    "    #             print(text_df.loc[text_df['Sentence_ID'] == sentence_id])\n",
    "                text_df.loc[(text_df['Sentence_ID'] == sentence_id+last_sentence) & \n",
    "                            (text_df['Word_ID'] >= match_idx_final[0]) & \n",
    "                            (text_df['Word_ID'] <= match_idx_final[-1]), 'Tag'] = tag\n",
    "            elif len(match_idx_final) == 1:\n",
    "    #             print(text_df.loc[(text_df['Sentence_ID'] == sentence_id) & \n",
    "    #                         (text_df['Word_ID'] == match_idx_final[0])])\n",
    "                text_df.loc[(text_df['Sentence_ID'] == sentence_id+last_sentence) & \n",
    "                            (text_df['Word_ID'] == match_idx_final[0]), 'Tag'] = tag     \n",
    "    last_sentence = text_df.iloc[-1]['Sentence_ID']+1\n",
    "    last_sentence_norm = text_df.iloc[-1]['Sentence_ID_Norm']+1\n",
    "    data_tidy_dict[handle] = text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, val in data_tidy_dict.items():\n",
    "#     val.to_csv(\"files/processed/{}.csv\".format(str(key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tidy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = data_tidy_dict['1033']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[temp['Sentence_ID'] == 463]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "df_concat = pd.concat(data_tidy_dict.values(), ignore_index=True)\n",
    "\n",
    "#change noised 'M' to 'O'\n",
    "stop_words = set(nltk.corpus.stopwords.words('english')) #179 words\n",
    "st = set(string.printable)\n",
    "df_concat[\"Word\"] = df_concat[\"Word\"].apply(lambda x: '' if set(str(x)) != set(str(x)).intersection(st) else x)\n",
    "df_m = df_concat.loc[df_concat['Tag']=='M'] \n",
    "df_m = df_m.loc[~df_m['Word'].isin(stop_words)] #remove stopwords\n",
    "df_m = df_m.loc[~df_m['Word'].str.contains('\\d', regex=True,na=False)] #remove numberish strings, remain Nan still as nan\n",
    "df_m_sort = df_m.groupby(['Word']).size().nlargest(200).reset_index(name='top200')\n",
    "#drop single characters\n",
    "droplist=[]\n",
    "for i in range(df_m_sort.shape[0]):\n",
    "    word = df_m_sort.iloc[i]['Word']\n",
    "    if word.lower() in stop_words or len(word)==1:       \n",
    "        droplist.append(i)\n",
    "df_m_sort.drop(droplist,inplace=True)\n",
    "top100_indicaters = df_m_sort['Word'].values.tolist()[:100]\n",
    "print(top100_indicaters)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.loc[(df_concat['Tag']=='M') & (~df_concat['Word'].isin(top100_indicaters)),'Tag']='O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_concat.loc[(df_concat['Word'] == 'NHANES') & (df_concat['Pub_id'] == '170')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat.loc[df_concat['Sentence_ID'].isin([2335])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat.to_csv(data_path+'/df_concat.csv')\n",
    "df_concat.to_csv(data_path+'/df_concat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
